\documentclass[a4paper,12pt]{article}

% Import the deliverable package from common directory
\usepackage{../common/deliverable}

% Tell LaTeX where to find graphics files
\graphicspath{{../common/logos/}{./figures/}{../}}

\usepackage{xspace}
\usepackage{lipsum}
\usepackage{pgfplots}
\usepackage{pgfplotstable}


% Set the deliverable number (without the D prefix, it's added automatically)
\setdeliverableNumber{1.2}

% Begin document
\begin{document}

% Create the title page with the title as argument
\maketitlepage{Report on deal.II improvements}

\newpage

% Main Table using the new environment and command
\begin{deliverableTable}
    \tableEntry{Deliverable title}{Report on deal.II improvements}
    \tableEntry{Deliverable number}{D1.2}
    \tableEntry{Deliverable version}{v1}
    \tableEntry{Date of delivery}{March, 31st 2025}
    \tableEntry{Actual date of delivery}{March, 31st 2025}
    \tableEntry{Nature of deliverable}{Report}
    \tableEntry{Dissemination level}{Public}
    \tableEntry{Work Package}{WP1}
    \tableEntry{Partner responsible}{UNIPI}
\end{deliverableTable}

% Abstract and Keywords Section
\begin{deliverableTable}
    \tableEntry{Abstract}{This report details the progress made during the first semester (Months 1-6) of the dealii-X project in enhancing the deal.II finite element library. The focus is on improvements for exascale readiness, specifically in high-performance matrix assembly, matrix-free GPU integration, as well as the initial performance evaluation of the polygonal discretization module (to be integrated in the library). Furthermore, this report outlines other relevant enhancements to deal.II undertaken within the project's work packages during this period.}
    \tableEntry{Keywords}{Keyword 1; Keyword 2; Keyword 3; Keyword 4; Keyword 5}
\end{deliverableTable}

\newpage

\begin{documentControl}
    \addVersion{0.1}{March 17, 2025}{Luca Heltai}{Initial draft}
    \addVersion{0.2}{March 24, 2025}{Luca Heltai}{Gathered information from all partners}
\end{documentControl}

\subsection*{{Approval Details}}
Approved by: M. Kronbichler \\
Approval Date: [Date]

\subsection*{{Distribution List}}
\begin{itemize}
    \item [] - Project Coordinators (PCs)
    \item [] - Work Package Leaders (WPLs)
    \item [] - Steering Committee (SC)
    \item [] - European Commission (EC)
\end{itemize}

\vspace*{2cm}

\disclaimer

\newpage

\tableofcontents % Automatically generated and hyperlinked Table of Contents

\newpage

\section{Introduction}
    \subsection{Overview of the dealii-X Project and Objectives}

The dealii-X project is a pioneering initiative dedicated to developing a
high-performance and scalable computational platform based on the deal.II finite
element library. The project directly addresses the
HORIZON-EUROHPC-JU-2023-COE-03-01 topic, specifically focusing on the
``Personalised Medicine / Digital twin of the human body'' as an Exascale
Lighthouse application area. The overarching goal of dealii-X is to advance
existing pre-exascale digital twin applications for human organs, such as some
deal.II based applications dedicated to the simulation of the brain, heart,
lungs, liver, and cellular interactions, to achieve exascale readiness. 

The project aims to enable real-time simulations of intricate biological
processes, thereby contributing to personalized medicine and cutting-edge
healthcare research. Ultimately, this enhanced simulation capability holds the
potential to significantly improve medical diagnostics and treatment planning.

\subsection{Objectives of Work Package 1 (WP1)}

The main objective of Work Package 1 (WP1) is to serve as the foundation
for the dealii-X Centre of Excellence by enhancing and expanding the
capabilities of the deal.II library to address the challenges of exascale
computing and facilitate the creation of advanced digital twins of human
organs.

The key steps of WP1 include:
\begin{itemize}
    \item Extending and improving the exascale capabilities of deal.II;
    \item Improving pre-exascale modules of the deal.II library;
    \item Developing an experimental polygonal discretization module for deal.II;
    \item Integrating PSCToolkit within deal.II;
    \item Integrating MUMPS within deal.II.
\end{itemize}

Specifically, the sub-work packages aim to:
\begin{itemize}
    \item \textbf{WP1.1 (Lead RUB)}: Develop matrix-free computational methods optimized for GPU architectures and enhance the scalability of solvers;
    \item \textbf{WP1.2 (Lead UNIPI)}: Improve the gmsh API, develop a generalized interface for coupling operators, enhance reduced order modelling capabilities, integrate low-rank approximation methods, and develop block preconditioners;
    \item \textbf{WP1.3 (Lead SISSA)}: Introduce and parallelize polygonal discretization methods within deal.II and develop related multigrid techniques;
    \item \textbf{WP1.4 (Lead UNITOV)}: integrate PSCToolkit into deal.II, leveraging GPU computing and developing efficient preconditioners for multiphysics problems;
    \item \textbf{WP1.5 (Lead INPT)}: Integrate the MUMPS solver directly into deal.II for use in multigrid methods and explore low-rank and mixed-precision techniques;
\end{itemize}

In summary, WP1 is dedicated to developing and integrating fundamental software components within the deal.II library and external libraries, with a strong emphasis on enabling exascale computation for the digital twin applications in WP2.

\subsection{Purpose and Scope of this Report (Deliverable D1.2)}

The purpose of this initial report on Work Package 1 is to provide a
comprehensive overview of the foundational activities and progress achieved
during the early stages of the dealii-X project. Specifically, it aims to
document the strategies and initial advancements made in enhancing the deal.II
library to meet the demands of exascale computing and to support the development
of sophisticated digital twins of human organs. This report serves to establish
a baseline understanding of the technical developments within WP1, including the
optimization of computational methods, the integration of key external
libraries, and the exploration of novel discretization techniques. The scope of
this report encompasses the activities undertaken within each of the sub-work
packages of WP1. 

The report will highlight the initial outcomes, any challenges encountered, and
the solutions identified, thus providing a clear picture of the progress towards
achieving the objectives of WP1 and its contribution to the overall goals of the
dealii-X project. It will serve as an initial benchmark for assessing the
development of the exascale building blocks and support tools for the subsequent
work packages.

A quantitative assessment of the overall advancement of WP1 will be provided in
the form of a list of pull requests (PRs) and issues opened in the deal.II
GitHub repository, along with a summary of the improvements made in satellite
repositories.

\section{Improvements for Exascale Readiness}

Work package 1.1 focuses on enabling efficient finite element computations
with the deal.II finite element library, where matrix-free evaluation
techniques and multigrid methods are the core scientific components.

The activities of the group at RUB can be summarized as follows:
\begin{itemize}
\item Benchmarks to assess the current performance of the core components.
\item Exascale readiness of Raviart--Thomas finite element algorithms for
  high-performance computing.
\item Restart capabilities in application solver ExaDG.
\item Initial setup of a domain decomposition infrastructure in the deal.II library to establish additional solver paradigms for the exascale era.
\end{itemize}

\subsection{Benchmarks for GPU systems}

In order to assess the core matrix-free algorithms on GPU systems, we have
worked towards a set of benchmarks.

\pgfplotstableread{
 5   6          8        1331  3.79912e-05   4.2353e+07  3.14263e-05  2.54234e-05   4.2e6   96  1.84954e-05
 5   6         16        2541  4.29821e-05  7.07614e+07  3.59094e-05  2.88131e-05   8.3e6  100  2.24764e-05
 5   6         32        4851  4.43232e-05  1.27189e+08  3.81402e-05  3.10807e-05   1.3e7  100  2.42564e-05
 5   6         64        9261  4.99301e-05  2.16052e+08  4.28646e-05  3.67775e-05   2.2e7  100  2.80755e-05
 5   6        128       18081   5.1163e-05  4.13633e+08  4.37127e-05  3.80174e-05   4.1e7  100  2.86181e-05
 5   6        256       35301  5.32898e-05  7.85706e+08   4.4929e-05  3.95952e-05   8.2e7  100  3.15277e-05
 5   6        512       68921   5.6458e-05   1.4457e+09  4.76731e-05  4.35147e-05   1.4e8  100  3.26065e-05
 5   6       1024      136161  5.64253e-05  2.87537e+09  4.73542e-05  4.09125e-05   2.6e8  100  3.20385e-05
 5   6       2048      269001  7.93226e-05  3.90264e+09  6.89279e-05  6.07493e-05   2.7e8  100  5.07143e-05
 5   6       4096      531441  0.000129025  4.39204e+09  0.000121001  0.000103359   2.7e8  100  9.24265e-05
 5   6       8192     1056321  0.000222163  5.00917e+09  0.000210877  0.000176216   2.7e8  100  0.000166144
 5   6      16384     2099601  0.000443032  5.53296e+09  0.000379472  0.000332482   2.8e8  100  0.000314249
 5   6      32768     4173281  0.000864378  5.63024e+09  0.000741226  0.000662211   2.7e8  100  0.000615349
 5   6      65536     8320641   0.00170104  5.66888e+09   0.00146777   0.00128264   2.7e8  100   0.00122682
 5   6     131072    16589601   0.00572042  2.96308e+09   0.00559876   0.00316049   2.5e8  100   0.00241532
 5   6     262144    33076161     0.019618   2.1394e+09    0.0154605   0.00896981   2.5e8  100   0.00542828
 5   6     524288    66049281    0.0462221   2.1247e+09    0.0310864    0.0183618   2.6e8  100    0.0114346
 5   6    1048576   131892801    0.0926743  2.14576e+09    0.0614668    0.0364508   2.6e8  100    0.0238106
 5   6    2097152   263374721     0.182145  2.18134e+09      0.12074    0.0721931   2.6e8  100    0.0467826
}\tableBPFiveAMD
\pgfplotstableread{
nele        ampere 
256         0.85  
512         1.65  
1024        2.75  
2048        3.38  
4096        4.07  
8192        4.62  
16384       4.93  
32768       5.18  
65536       5.20  
131072      5.22 
262144      5.23 
524288      5.24  
1048576     nan   
}\tableBP

\pgfplotstableread{
 4   5        256       18785  0.000372951  5.03686e+07    99  0.000251866
 4   5        384       28033  0.000408611  6.86056e+07   100  0.000289283
 4   5        512       36465  0.000488475  7.46507e+07   100  0.000365702
 4   5        768       54417  0.000611396  8.90046e+07   100  0.000490703
 4   5       1024       70785  0.000735216  9.62778e+07   100  0.000605897
 4   5       1536      105633    0.0010361  1.01953e+08   100  0.000898252
 4   5       2048      140481   0.00128128  1.09641e+08   100   0.00114143
 4   5       3072      210177    0.0020391  1.03074e+08   100   0.00189293
 4   5       4096      276705   0.00254333  1.08796e+08   100   0.00238997
 4   5       6144      413985   0.00367827  1.12549e+08   100   0.00351821
 4   5       8192      545025   0.00477614  1.14114e+08   100   0.00461896
 4   5      12288      815425   0.00699715  1.16537e+08   100   0.00681141
 4   5      16384     1085825   0.00921008  1.17895e+08   100   0.00899371
 4   5      24576     1626625    0.0137157  1.18595e+08   100    0.0134318
 4   5      32768     2154945    0.0183288  1.17572e+08   100    0.0179785
 4   5      49152     3228225    0.0273663  1.17964e+08   100    0.0268812
 4   5      65536     4276737    0.0363368  1.17697e+08   100    0.0357235
 4   5      98304     6406785    0.0547228  1.17077e+08   100    0.0538665
 4   5     131072     8536833    0.0724943  1.17759e+08   100    0.0714299
 4   5     196608    12796929     0.109293  1.17088e+08   100     0.107686
  }\tableMFKokkos

\pgfplotstableread{
 128  0.06204099359     0.06122035118     0.9288824383     0.9893643334     0.6850488097     0.04748168987     0.08408660921     1.34873135     1.354440024     1.185448618     1.266223488
 256  0.1201602638     0.1235158795     1.756697409     1.912388693     1.250928423     0.09173891106     0.1659966282     2.626395272     2.534653465     2.312472901     2.411999699
 512  0.2421426599     0.2463262745     3.462079411     3.66028024     2.112211221     0.1840169756     0.3358874777     4.959318094     4.842615012     4.368600683     4.669147151
 1024  0.4796846074     0.4812030075     6.195247084     6.339772164     2.836879433     0.355389707     0.6525252216     7.599145096     7.74677722     7.532070142     7.880802857
 2048  0.9150766019     0.946119248     9.304015991     10.13018875     3.588852128     0.7014505779     1.292504986     11.9108547     11.61313736     10.93689922     12.17192849
 4096  1.848021859     1.848695803     13.6406021     14.23725043     4.247517442     1.383828665     2.567600096     15.72240135     15.63120134     14.63275221     16.29379754
 8192  3.602500642     3.753184159     16.65257269     17.67498058     4.526346964     2.764735582     5.107996209     17.45742196     18.29192047     16.59024999     19.70215877
 16384  2.806802109     4.818063976     19.18087906     20.10129167     4.568226734     2.156073168     9.049493838     19.50290449     19.9912148     18.52203562     21.74143825
 32768  2.820670501     6.145351977     20.67590419     21.60462896     4.627798881     2.521856914     9.146450391     20.6549474     21.20686531     19.65205876     23.18538687
 65536  2.461917958     9.090636711     21.5591429     22.51446318     4.658868143     2.419041685     12.35685238     21.31407296     21.81101733     20.26488821     24.04673144
 131072  2.030735197     9.311207093     22.02793019     22.97760861     4.68662755     1.884650075     12.40399284     21.64933469     22.14215632     20.59101129     24.49376219
 262144  1.82988666     9.352695193     22.265483     22.97002219     4.695679339     1.477229265     11.6228092     21.81543165     22.31424488     20.75276921     24.71834678
 524288  1.830664127     8.872589741     22.39760604     23.34501724     4.703506688     1.462271549     10.67090272     21.90104228     22.40858047     20.83938828     24.57966544
 1048576  1.806746246     9.19572938     22.4594457     23.41187244     4.704535319     1.449557117     10.92101941     21.94892494     22.45115915     20.87924376     24.9059036
}\tableLowLevel

As a benchmark, we consider a matrix-free operator using the CEED benchmark
BP5\footnote{\url{https://ceed.exascaleproject.org/bps/}, see the benchmark
  description at \url{https://doi.org/10.1177/1094342020915762}}, which uses a
3D Poisson equation on a mesh of deformed elements and solving the linear
system with a conjugate gradient iterative solver and diagonal
preconditioner. We list the throughput in terms of degrees of freedom (DoFs)
per second and CG iteration. Figure~\ref{fig:bp5} lists the measured
performance in February 2025. The CPU code uses advanced data locality
optimizations established in our previous
research\footnote{\url{https://doi.org/10.1177/10943420221107880}}, and
therefore the CPU codes reach a relatively high throughput despite a formally
much slower memory subsystem compared to the A100, where the CUDA code reaches
1.1 TB/s of memory bandwidth. The Kokkos code currently integrated in deal.II
was found to be really slow in this experiment, reaching 0.12 GDoF/s with the
code from February 2025 vs. 5.2 GDoF/s with CUDA (code established in
2016--2022). Also for the CPU, the Kokkos abstractions give poor performance
with 0.26 GDoF/s vs. 3.6 GDoF/s.

\begin{figure}
  \centering
      \begin{tikzpicture}
    \begin{semilogxaxis}[
      title style={font=\scriptsize},
      tick label style={font=\scriptsize},
      title={Node-level GPU performance},
      label style={font=\scriptsize},
      legend style={font=\tiny},
      width=0.5\columnwidth,
      height=0.4\textwidth,
      xlabel={DoFs},
      ylabel={[billion DoFs $\times$ CG its] / [sec]},
      legend columns = 2,
      legend to name=legend:node,
      legend cell align={left},
      grid,
      semithick,
      ymin=0,ymax=7,ytick={0,1,2,3,4,5,6,7},
      xmin=1e4,xmax=1.7e8
      ]

      \addplot [red, solid, mark=triangle] table[x expr={\thisrowno{3}}, y expr={\thisrowno{3}/\thisrowno{7}*1e-9}] {\tableBPFiveAMD};
      \addlegendentry{CPU 2$\times$\,64C Zen 2};
      \addplot [green!50!black, dashed, solid, mark=square*] table[x expr={\thisrowno{0}*125}, y expr={\thisrowno{1}}] {\tableBP};
\addlegendentry{A100, CUDA};
      \addplot [red!50!black, solid] table[x expr={\thisrowno{3}}, y expr={\thisrowno{8}*1e-9}] {\tableBPFiveAMD};
\addlegendentry{CPU 2$\times$\,64C Zen 2, Kokkos};
      \addplot [black, solid, mark=x] table[x expr={\thisrowno{3}}, y expr={\thisrowno{5}*1e-9}] {\tableMFKokkos};
\addlegendentry{A100, Kokkos};

\end{semilogxaxis}
  \end{tikzpicture}
  \\
  \strut\hfill\pgfplotslegendfromname{legend:node}\hfill\strut
  \caption{Throughput of CEED BP5 benchmark case with deal.II using Kokkos and native CUDA/CPU implementations.}
  \label{fig:bp5}
\end{figure}

In order to better understand the performance gap and mitigate performance
differences, we have worked on more low-level sum Factorization benchmarks to
assess the performance of the Kokkos programming model in comparison. The
algorithm successively applies matrix-vector products on finite elements in 3D
with polynomial degree 5, with a focus on optimizing the use of shared memory
and ensuring coalesced memory access patterns. The objective of the benchmark
is to compare the throughput, measured in degrees of freedom per second, for
each programming model, highlighting performance differences between the
vendor-neutral Kokkos library and the native programming model
CUDA. Fig.~\ref{fig:sum_fact} shows the results of this experiment.

\begin{figure}
  \centering
      \begin{tikzpicture}
    \begin{semilogxaxis}[
      title style={font=\scriptsize},
      tick label style={font=\scriptsize},
      title={Node-level GPU performance},
      label style={font=\scriptsize},
      legend style={font=\tiny},
      width=0.5\columnwidth,
      height=0.4\textwidth,
      xlabel={DoFs},
      ylabel={[billion DoFs] / [sec]},
      legend columns = 2,
      legend to name=legend:new,
      legend cell align={left},
      grid,
      semithick,
      ymin=0,
      xmin=1e4,xmax=1.7e8
      ]

      \addplot table[x expr={\thisrowno{0} * 125}, y expr={\thisrowno{2}}] {\tableLowLevel};
      \addlegendentry{Kokkos, across elem};
      \addplot table[x expr={\thisrowno{0} * 125}, y expr={\thisrowno{4}}] {\tableLowLevel};
      \addlegendentry{Kokkos, per thread};
      \addplot table[x expr={\thisrowno{0} * 125}, y expr={\thisrowno{5}}] {\tableLowLevel};
      \addlegendentry{cuBLAS};
      \addplot table[x expr={\thisrowno{0} * 125}, y expr={\thisrowno{7}}] {\tableLowLevel};
      \addlegendentry{CUDA, across elem};
      \addplot table[x expr={\thisrowno{0} * 125}, y expr={\thisrowno{11}}] {\tableLowLevel};
      \addlegendentry{CUDA, per thread};

\end{semilogxaxis}
  \end{tikzpicture}
  \\
  \strut\hfill\pgfplotslegendfromname{legend:new}\hfill\strut
  \caption{Throughput evaluation of sum factorization kernels for forward evaluation (basis functions $\to$ values at quadrature points) with CUDA and Kokkos on an A100 GPU.}
  \label{fig:sum_fact}
\end{figure}

\subsection{Raviart--Thomas finite elements}

Another string of activities is centered around the application-readyness of Raviart--Thomas elements, where future requirements and potential improvement opportunities are investigated with the ExaDG application code pioneering the use of Raviart-Thomas elements for high-Reynolds incompressible flow problems in a matrix-free setting based on deal.II.

This will also require some debugging in the deal.II implementation of the
Raviart--Thomas elements, as these code paths are quite new and have not been
extensively used by the community yet due to the fact that the use of
Raviart--Thomas elements has significantly increased in the last
years. Currently, the time integration and matrix-free solver capabilities in
ExaDG have been extended, and further testing regarding the performance will
be executed in the near future on the JUPITER system.
\begin{itemize}
\item Improvements of Raviart-Thomas incompressible flow solvers in ExaDG:\\
  \href{https://github.com/kronbichler/exadg/tree/martin}{https://github.com/kronbichler/exadg/tree/martin}
\end{itemize}

\subsection{Restart capabilities}

To reduce simulation costs for large-scale flow problems, the restarting capabilities of ExaDG have also been extended, while the general setup and software design have been kept general, solely based on deal.II data structures for maximal reusability. The main goal here is to allow for matrix-free mesh-to-mesh projection, such that performance studies of various solver settings and performance tuning can start from a precursor run to face the target physics directly without the need for recomputing the startup phase. This has been realized in a general setting, splitting off the grid-to-grid projection aspect and the changes with respect to the time integrators within ExaDG. Hence, this code can be easily transferred to other codes as the required routines are based solely on deal.II. This naturally allows considering arbitrary physics and discretizations, various finite elements (polynomial degree, cell type, continuity, etc.), or non-matching grids.
    \begin{itemize}
        \item Grid-to-grid projection:\\
        \href{https://github.com/exadg/exadg/pull/735}{https://github.com/exadg/exadg/pull/735}    
        \item Restarting including grid-to-grid projection and rollout to all time integrators (implicit and explicit, various single step and multi step schemes) and physics (convection-diffusion, Navier-Stokes equations for compressible and incompressible flow, acoustic conservation equations) in ExaDG:\\
        \href{https://github.com/exadg/exadg/pull/729}{https://github.com/exadg/exadg/pull/729}\\
        \href{https://github.com/exadg/exadg/pull/731}{https://github.com/exadg/exadg/pull/731}\\
        \href{https://github.com/exadg/exadg/pull/732}{https://github.com/exadg/exadg/pull/732}\\
        \href{https://github.com/exadg/exadg/pull/734}{https://github.com/exadg/exadg/pull/734}     
    \end{itemize}

\section{Improvement of pre-exascale modules of the deal.II library}

Work package 1.2 focuses on enhancing the existing modules of the deal.II
library to prepare them for the challenges of exascale computing. This includes
several key activities: 
\begin{itemize}
    \item Improving the gmsh API to support large-scale;
    \item Developing a generalized interface for coupling operators, which
    is fundamental for multiphysics and multiscale simulations, including methods
    for conforming and non-conforming grids and non-local coupling techniques;
    \item Enhancing the reduced-order modeling (ROM) capabilities of deal.II,
    implementing algorithms for data analysis and reduced-order geometric modeling;
    \item Integrating low-rank approximation methods within deal.II, including
    low-rank and hierarchical low-rank solvers and preconditioners to tackle
    large-scale computational problems;
    \item Developing block preconditioners for coupled problems, which are
    essential for the stability and efficiency of simulations.
\end{itemize}
 
The overall objective is to improve the pre-exascale functionalities of deal.II,
making it an even more robust and efficient library for a wide range of
scientific and engineering applications aiming for the use of exascale
computers.

\subsection{Extension of current gmsh API for exascale simulations}
    The key researcher in this task is a postdoc researcher, Dr. Devi Raksha, who
    has been hired on the project, and will start her position in April 2025.
    The task is to extend the gmsh API to support large-scale simulations, with
    a focus on biomedical applications, and to improve the gmsh deal.II
    interface to enable the generation and handling of large-scale meshes.

    The task has not been started yet, and it is postponed to the second
    semester of the project.

\subsection{Generalized Interface for Coupling Operators in deal.II}
    \subsubsection{Objectives and Planned Activities (Task WP1.2.2)}

    The objective of this task is to develop a generalized interface for
    coupling operators in deal.II, which is essential for multiphysics and
    multiscale problems. The focus is on two different types of coupling:
    \begin{itemize}
        \item Coupling between different physical processes, such as fluid-structure interaction or heat transfer in porous media;
        \item Coupling between different discretization schemes, including
        non-matching and non-overlapping discretizations.
    \end{itemize}

    This subtask was supposed to start on the second semester of the project,
    but an early version of the generalized interface has been developed and
    merged into the deal.II repository in April 2024, before the official start
    of the dealii-X project, making it more convenient to start the development
    of the rest of this task in the first semester.

    The developed approach encompasses two main aspects: a high-level interface
    for defining coupling operators and a low-level interface for implementing
    efficient evaluation of coupled bilinear forms (on possibly non-matching or
    non-nested grids). The high-level interface enables the computation of
    abstract bilinear forms for coupling between geometric entities, supporting
    various kernels and use cases like BEM and bulk-surface coupling. The low
    level interface needs to be flexible enough to allow the use of different
    discretization schemes, including non-conforming discretizations, and
    possibly different parallel distributions of the meshes.

    \subsubsection{Implementation Details} 

    The two pull requests, ``FECouplingValues''
    (\url{https://github.com/dealii/dealii/pull/15773}) and
    ``FEValuesViews::RenumberedView''
    (\url{https://github.com/dealii/dealii/pull/15819}), directly address the
    objective of developing a ``Generalized interface for coupling operators in
    deal.II'', as outlined in Work Package 1.2.2 of the dealii-X project, by
    providing the high level interface discussed in the introduction.

    Pull request \#15773 introduces a new class called `FECouplingValues`, specifically designed for the computation of general coupling operators. The primary goal of this class is to enable the computation of abstract bilinear forms of the type:
    $$ \int_{T_1} \int_{T_2} K(x_1, x_2) f(x_1) g(x_2) \, dT_1 \, dT_2, $$ where
    $T_1$ and $T_2$ represent two arbitrary sets (cells, faces, edges, or their
    combinations), and $K$ is a coupling kernel (potentially singular). The
    class is designed to support various types of kernels $K$. The pull request
    includes several tests demonstrating how the class can be used for different
    types of coupling:
    \begin{itemize}
        \item Coupling for Boundary Element Methods (BEM);
        \item As a replacement for `FEInterfaceValues`, integrating
        discontinuous Galerkin flux terms;
        \item For bulk-surface coupling computations.
    \end{itemize}


    Pull request \#15819 introduces the class `FEValuesViews::RenumberedView`.
    This component was developed as part of the work for \#15773 and is intended
    for use by `FECouplingValues`. `RenumberedView` allows the creation of a
    reordered view of an `FEValuesBase` object, where both degrees of freedom
    and quadrature points are renumbered according to provided renumbering
    vectors.

    In summary, these two pull requests jointly implement the "Generalized interface for coupling operators in deal.II" envisioned in WP1.2.2:
    \begin{itemize}
        \item `FECouplingValues` provides the framework for defining and
        computing general coupling operators between different geometric
        entities and discrete spaces. This addresses the need for a versatile
        interface to manage various coupling techniques, including methods for
        non-matching grids and non-local coupling.
        \item `FEValuesViews::RenumberedView` provides a fundamental tool for managing the correspondence (or lack thereof) between the degrees of freedom and quadrature points of the different spaces being coupled via `FECouplingValues`.
    \end{itemize}

    The integration of these two functionalities into the deal.II library
    provides users with powerful and flexible tools to tackle complex coupling
    problems essential for the multiphysics and multiscale simulations at the
    core of the dealii-X project.

    The two pull requests were merged into the main deal.II repository on April
    11, 2024, and included in the Release 9.6 milestone, before the official
    start of the dealii-X project. As part of the WP1.2 tasks, a biologically
    relevant application that uses the new functionality is being developed and
    will be integrated in the deal.II repository as a new tutorial program. The
    tutorial will demonstrate the use of the new `FECouplingValues` class for
    the computation of a bulk-surface coupling operator.

    A low level interface for coupling operators has been developed and merged
    into the deal.II repository in pull request \#17919, which introduces a new
    abstract base class `MGTwoLevelTransferBase` to enable non-nested multigrid
    transfers. The primary goal of this class is to generalize the interface for
    managing transfers between non-nested grids, which is a crucial step towards
    addressing the challenges of coupling operators on non-matching grids. 

    The `MGTwoLevelTransferBase` class provides a flexible framework for
    implementing matrix-free transfer operators between different levels of a
    multigrid hierarchy. This development aligns with the objectives of WP1.2.2,
    as it contributes to the creation of a generalized interface for coupling
    operators in deal.II. Additionally, the techniques developed for non-nested
    multigrid transfers could offer valuable insights and algorithms for
    tackling coupling problems on non-matching grids in more general contexts.

    The implementation of this low-level interface is expected to enhance the
    scalability and efficiency of multigrid methods in deal.II, particularly for
    applications involving complex grid hierarchies or non-conforming
    discretizations. This work also complements the high-level interface
    provided by `FECouplingValues`, creating a comprehensive toolkit for
    addressing a wide range of coupling and multigrid challenges in the dealii-X
    project. The work has been submitted, and is currently under review. A
    preprint is availabele at ~\cite{FederEtAl2024b}.

   A tutorial program that illustrates the use of the non-nested multigrid
   transfer has been proposed and is currently under review in the deal.II
   library
    \url{https://github.com/dealii/dealii/pull/17919}.


\section{Polygonal Discretization Methods}

Work Package 1.3 of the dealii-X project is dedicated to the introduction and
development of an experimental module for polygonal discretization within the
deal.II library. 

\subsection{Objectives and Planned Activities (Task WP1.3.1)}

The primary objective of this sub work package is the development of a polygonal
discretization module within the deal.II library. The deal.II library currently
supports a wide range of finite element methods, defined on standard simplicial
and hexahedral meshes, but does not support polygonal and polyhedral (\emph{polytopic}) meshes.

Planned activities include:
\begin{itemize}
    \item Develop data structure for polytopic meshes obtained by agglomeration of standard deal.II meshes;
	\item Exploit  R-tree data structures for efficient generation of balanced and nested hierarchies of  polytopic meshes and test use of such hierarchies of  polytopic meshes to enhance multigrid solvers;
	\item Develop extensive library of examples of polytopic finite element methods for complex multiscale problems.  
\end{itemize}

 \subsection{Progress and Current Status}

A Preliminary version of the polygonal discretization module has been developed
as part of this work package, and it is currently available in the repository
\url{https://github.com/fdrmrc/Polydeal}. The module is designed to be
integrated into the deal.II library, and its integration is expected to be
completed by the end of the project.

The results of this early stage of the project are presented in the article ``R3MG:
R-tree based agglomeration of polytopal grids with applications to multilevel
methods''~\cite{FederEtAl2025}, that clearly shows the potential of such
discretization methods. 

This work introduces a novel approach for the agglomeration of standard deal.II
grids into polygonal and polyhedral meshes, based on spatial indices,
specifically the R-tree data structure. The ``R3MG'' article demonstrates how
building an R-tree spatial database from an arbitrary fine mesh offers a natural
and efficient agglomeration strategy with several key features: 

\begin{itemize}
    \item 
    The process is fully automated, robust, and dimension-independent;
    \item It automatically generates a
balanced and nested hierarchy of agglomerates. This is a crucial property for
the subsequent application of multigrid methods;
\item The shape of the agglomerates is closely aligned with their axis-aligned bounding boxes and thus are characterised by good and easy to assess  shape-regularity properties.
\end{itemize}

A fundamental aspect of this approach is the ability to automatically extract
nested sequences of agglomerated meshes, which can be directly used within
multigrid solvers. This is particularly relevant for WP1.3, as one of its
objectives is the development of agglomeration-based multigrid methods for
single-physics problems. The R-tree based approach, named R3MG (R-tree based
MultiGrid), is proposed as a multigrid preconditioning technique with
Discontinuous Galerkin methods. The experiments presented in the article, based
on polygonal Discontinuous Galerkin methods, confirm the effectiveness of the
approach in the context of complex three-dimensional geometries relevant to biomedical applications and in the
design of geometric multigrid preconditioners, at least for model problems.

Furthermore, the ``R3MG'' article highlights how the R-tree-based method preserves
mesh quality and significantly reduces the computational cost associated with
the agglomeration process, favorably comparing it with graph partitioning tools
like METIS. In particular, it is demonstrated that R-tree-based agglomeration
preserves structured meshes, a property not shared by METIS. The ability to
generate nested grid hierarchies through R-tree agglomeration simplifies the use
of simpler and more economical intergrid transfer operators compared to the
non-nested case. In summary, the ``R3MG'' article represents a significant
contribution to the objective of WP1.3 to develop an experimental polygonal
discretization module in deal.II, providing an efficient and automated
methodology for generating R-tree based agglomerated polygonal grid hierarchies,
specifically designed for application with Discontinuous Galerkin methods and
multigrid techniques.



 \subsection{Challenges and Future Plans}

The repository \url{https://github.com/fdrmrc/Polydeal} is being enriched with a range of examples for comprehensive benchmarking of both polytopic discretisation methods and multigrid solvers, before integration within the deal.II library. This include the implementation of polytopic discontinuous Galerkin methods for fluid flow problems and a multigrid solver for accelerating the solution of complex electrophysiological models.

\section{Integration of PSCToolkit into deal.II (WP1.4)}
    \subsection{Objectives and Planned Activities}
        The primary objective of WP1.4 is to integrate the PSCToolkit (Parallel Sparse Computing Toolkit) into the deal.II library. This integration aims to leverage GPU computing to enhance performance and develop efficient preconditioners for multiphysics problems. PSCToolkit provides advanced routines for sparse matrix operations optimized for GPU architectures, which are critical for large-scale simulations.

        Planned activities include:
        \begin{itemize}
            \item Developing a GPU-accelerated interface for PSCToolkit within deal.II;
            \item Implementing efficient preconditioners tailored for multiphysics problems;
            \item Benchmarking the performance of the integrated toolkit on exascale systems.
        \end{itemize}

        \subsection{Progress and Current Status}
        Initial efforts have focused on designing the interface between PSCToolkit and deal.II's existing data structures. A pull request has been opened in the deal.II repository to support detection and linkage with the PSBLAS library (one of the two core libraries identified for interfacing PSCToolkit with deal.II; the other is AMG4PSBLAS, whose integration will follow in a subsequent pull request) \url{https://github.com/dealii/dealii/pull/18199}. A prototype of the interface has been planned in collaboration with the MUMPS team to ensure seamless integration with deal.II's current data structures of both interfaces.

        Several enhancements have been made to the PSCToolkit library itself to facilitate its integration with external libraries (such as deal.II), including:
        \begin{itemize}
            \item Fine control and tuning of compile definitions to simplify the detection of the library and its components, including support for external libraries, ensuring compatibility at linking time with deal.II and its dependencies;
            \item Adding support for CMake to streamline integration with external libraries;
            \item Exporting the library's macro definitions in a dedicated header file to simplify detection of the library and its components, regardless of the build system used;
            \item Adding PSBLAS to the Spack package manager to simplify the installation of the library and its dependencies.
        \end{itemize}

        \subsection{Challenges and Future Plans}

        The integration of PSCToolkit into deal.II is still in its early stages, requiring careful planning to ensure compatibility and performance. The primary challenge lies in ensuring that the interface between PSCToolkit and deal.II is robust, efficient, and seamlessly communicates with existing deal.II data structures without introducing additional ones. This challenge stems from the proliferation of slightly incompatible data structures in existing external linear algebra packages integrated with deal.II, which has led to significant maintenance efforts and user confusion. Addressing this issue is a key focus to ensure a streamlined and user-friendly integration.

\section{Integration of MUMPS Solver into deal.II (WP1.5)}
    \subsection{Objectives and Planned Activities}
    WP1.5 aims to integrate the MUMPS (Multifrontal Massively Parallel Sparse) solver directly into deal.II. This task focuses on enabling its use in multigrid methods and exploring advanced techniques such as low-rank approximations and mixed-precision computations to enhance solver efficiency.

    Planned activities include:
    \begin{itemize}
        \item Developing a direct interface between deal.II and MUMPS;
        \item Implementing low-rank approximation techniques to reduce computational costs;
        \item Exploring mixed-precision strategies to optimize performance on modern hardware.
    \end{itemize}

    \subsection{Progress and Current Status}
    A preliminary interface for MUMPS was present in the deal.II library until
    version 9.2. Such interface had been removed, in favour of using MUMPS
    through external linear algebra packages such as PETSc and Trilinos.
    
    A careful planning of the interface between deal.II and MUMPS is currently
    ongoing, with the goal of ensuring that the interface is robust and efficient,
    and that it seamlessly communicates with existing deal.II data structures, extending, integrating, and modernizing the former MUMPS interface in deal.II. An initial pull request
    has been opened in the deal.II repository to support detection and linkage
    with the MUMPS library, reverting the removal of the MUMPS interface in deal.II, and re-introducing the original MUMPS interface, including some simple tests to ensure that the interface is working correctly. The pull request is available at
    \url{https://github.com/dealii/dealii/pull/18255}.
    
    \subsection{Challenges and Future Plans}

    The integration of MUMPS into deal.II is currently working only with the
    serial version of the deal.II data structures, and it is still in its early
    stages, requiring careful planning to ensure compatibility and performance.
    Similarly to what has been discussed with the PSBLAS library, the primary
    challenge lies in ensuring that the interface between MUMPS and deal.II is
    robust, efficient, and seamlessly communicates with existing deal.II data
    structures. Ensuring compatibility and avoiding redundant data structures is
    crucial to streamline integration and reduce maintenance efforts.

\bibliographystyle{abbrv}
\bibliography{../common/bibliography.bib}
    
\label{MyLastPage}


\end{document}
